{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hkeNZTe418i"
   },
   "source": [
    "# Fake Political News Detection using CNN, DNN, LSTM\n",
    "\n",
    "## Overview\n",
    "This notebook contains step-by-step development and walkthough of fake news classification using Convolutional Neural Network, Deep Neural Network, and Long Short-Term Memory (Recurrent Neural Network) architecture. It includes the end-to-end design that covers the 6Ds of creating AI-enabled systems. \n",
    "\n",
    "**_NOTE_**: As the Golve Embedding file is larger then 100MB it can not be uploaded to github, but it can be downloaded from this [link](https://nlp.stanford.edu/projects/glove/?msclkid=17352c13cf3111ecb36fc6395436b39d). Download glove.6B.zip  and extract glove.6B.50d.txt from the zip. Put the file int he same directory as this jupyter lab notebook directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5K5S9CSBP8u"
   },
   "source": [
    "## Decomposition\n",
    "\n",
    "\n",
    "Receiving latest news in the model era has become much easier due to the technology. This raises the concern for the validity of the news. Even if the news is fake it will also spread much quicker and before it is proven to be fake, it will have done some damage. There are many types of news we could focus on such as sports, business, general, crime, but political news effects our day to day life more than any other type of news. After reading/hearing news some people research the topic to determine the validity of the news which is time consuming. Having a classification model validate the news could help speed up the research process or act as a middle man that validates the news before it is brought in front of the people.The simple principle behind this project is to decrease the effects the fake news has in the works and allow the users to gain confidence in the information they consume.<br><br>\n",
    "\n",
    "**Decomposition of the process**\n",
    "\n",
    "Validating news requires labeled dataset of real and fake news. Once the data is collected the next step is to train the model to learn human writting patterns. The last step is to deploy the product and make it available for the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v2iPSg3It8T"
   },
   "source": [
    "## Domain Expertise\n",
    "\n",
    "This project would require policitcal experts for news labeling as well determining how the fake news can be detected. This would allow the AI developers to understand what kind of patterns the model needs to extract to be able to validate the news. The false positive classification could lead to the change in the environment around user such as the early retirement of a political leader or sudden change in the stock prices which leads to product prices going up. False negative would lead to similar effects like voting for the wrong official. The false negetive classification and false positive classification both could damage the economy equally so accuracy is very important to gain the trust of the user in the model.<br><br>\n",
    "\n",
    "\n",
    "**How does an AI detect fake news?**\n",
    "\n",
    "Fake news does not always have taletell signs. In some cases, we could find some grammatical, and spelling mistakes which could be an indication of fake news. An AI clould be developed by feeding it huge virtual data libraries that contain authentic information and sources to help AI learn the patterns of human writing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgdJn64WUBM9"
   },
   "source": [
    "## Data\n",
    "Data collection for this project is easier as there are huge labeled virtual data libraries containing news articles to train the AI. We can also leverage online resources such as [FactCheker.org](https://www.factcheck.org/), [Snopes.com](https://www.snopes.com/fact-check/), [Google Fact Check Tool](https://www.snopes.com/fact-check/), [politifact.com](https://www.politifact.com/), etc... </br></br> \n",
    "\n",
    "For this project we will be utalizing [Kaggle Fake & Real News Dataset](https://www.kaggle.com/datasets/hassanamin/textdb3) which is a dataset of 6335 news articles with 50% real and 50% fake news. The dataset contains four columns: Serial number (starting from 0); Title (about the text news heading); Text (about the news content); and Label (FAKE, REAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNYsdwW4a7Is",
    "outputId": "aa3a417b-9e3b-45d1-8148-50c05901ef98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import general packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import pickle, os, json, re, requests\n",
    "\n",
    "#import sklearn packsges\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#import nltk packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# import keras packages\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dense, Input, Flatten, Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GlobalMaxPooling1D\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-MzKi2DdvqIa"
   },
   "outputs": [],
   "source": [
    "# set the flag for tensorflow CPU\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRZWTqCwzQyu",
    "outputId": "a31f8643-afde-4234-8bf7-bc29af70f439"
   },
   "outputs": [],
   "source": [
    "# path variables\n",
    "EMBEDDING_DIM = 50\n",
    "dataset_path = \"fake_or_real_news.csv\"\n",
    "embedding_path = \"glove.6B.50d.txt\"\n",
    "save_keras_model_path = 'FlaskServer/artifacts/keras_artifacts'\n",
    "save_tokenizer_path = 'FalskServer/artifacts/tokenizer.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8EnYp3yWwT-"
   },
   "source": [
    "### Explore & Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "lkRf4hlUa7qq",
    "outputId": "89e59b27-a592-4a56-a319-37b628bb0870"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-84b81a4c-41cd-4ea4-b6cb-4b1d625adc47\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>4490</td>\n",
       "      <td>State Department says it can't find emails fro...</td>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>8062</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>8622</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligarc...</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>4021</td>\n",
       "      <td>In Ethiopia, Obama seeks progress on peace, se...</td>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>4330</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6335 rows × 4 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84b81a4c-41cd-4ea4-b6cb-4b1d625adc47')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-84b81a4c-41cd-4ea4-b6cb-4b1d625adc47 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-84b81a4c-41cd-4ea4-b6cb-4b1d625adc47');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "0           8476                       You Can Smell Hillary’s Fear   \n",
       "1          10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2           3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3          10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4            875   The Battle of New York: Why This Primary Matters   \n",
       "...          ...                                                ...   \n",
       "6330        4490  State Department says it can't find emails fro...   \n",
       "6331        8062  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...   \n",
       "6332        8622  Anti-Trump Protesters Are Tools of the Oligarc...   \n",
       "6333        4021  In Ethiopia, Obama seeks progress on peace, se...   \n",
       "6334        4330  Jeb Bush Is Suddenly Attacking Trump. Here's W...   \n",
       "\n",
       "                                                   text label  \n",
       "0     Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1     Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2     U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3     — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4     It's primary day in New York and front-runners...  REAL  \n",
       "...                                                 ...   ...  \n",
       "6330  The State Department told the Republican Natio...  REAL  \n",
       "6331  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...  FAKE  \n",
       "6332   Anti-Trump Protesters Are Tools of the Oligar...  FAKE  \n",
       "6333  ADDIS ABABA, Ethiopia —President Obama convene...  REAL  \n",
       "6334  Jeb Bush Is Suddenly Attacking Trump. Here's W...  REAL  \n",
       "\n",
       "[6335 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "N-zVn2kwaDC-",
    "outputId": "d86ab2f3-2a35-49d5-a327-35254db3ac06"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1f64877c-485a-4dab-a0d5-8b6fab6e1467\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6335 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f64877c-485a-4dab-a0d5-8b6fab6e1467')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1f64877c-485a-4dab-a0d5-8b6fab6e1467 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1f64877c-485a-4dab-a0d5-8b6fab6e1467');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                   text label\n",
       "0     Daniel Greenfield, a Shillman Journalism Fello...  FAKE\n",
       "1     Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE\n",
       "2     U.S. Secretary of State John F. Kerry said Mon...  REAL\n",
       "3     — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE\n",
       "4     It's primary day in New York and front-runners...  REAL\n",
       "...                                                 ...   ...\n",
       "6330  The State Department told the Republican Natio...  REAL\n",
       "6331  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...  FAKE\n",
       "6332   Anti-Trump Protesters Are Tools of the Oligar...  FAKE\n",
       "6333  ADDIS ABABA, Ethiopia —President Obama convene...  REAL\n",
       "6334  Jeb Bush Is Suddenly Attacking Trump. Here's W...  REAL\n",
       "\n",
       "[6335 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop id, title colmns\n",
    "df = df.drop(['Unnamed: 0', 'title'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NU81BKvca8V1",
    "outputId": "c7e10888-fbd6-4326-8f76-db2b10d1615b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "HKG_CG3GdKfT",
    "outputId": "4df7135b-cc8b-4b7d-9d34-bca095532f8f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# drop null values\n",
    "df = df.dropna()  \n",
    "      \n",
    "# plot class distribution\n",
    "sns.countplot(x = df['label'])\n",
    "for index, value in enumerate([df['label'].value_counts()[0], df['label'].value_counts()[1]]):\n",
    "    plt.text(index, value,  str(value))\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky9qgmwhczqK"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "The below function removes stopwords, and punctuations. It also applies PotterStemmer,  world net lemmatization, and sets the final string as lower case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RpsmT7J1dcD4"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def pre_process_news(news):\n",
    "    \"\"\"\n",
    "      The function applies the WordNetLemmatization , PotterStamming, stop word removal, and punctuation removal on the string for preprocessing.\n",
    "\n",
    "       Parameters: \n",
    "           news: string that needs to be pre-processed\n",
    "\n",
    "       Returns:\n",
    "           processed string\n",
    "    \"\"\"\n",
    "    \n",
    "    lemm = WordNetLemmatizer()\n",
    "    ps = PorterStemmer() \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    news = news.lower()\n",
    "    news = re.sub(\"[^0-9a-z]\", ' ' , news) # removes punctuations\n",
    "    tokenized = news.split(\" \")\n",
    "    news = [ ps.stem(word) for word in tokenized if word not in stop_words] # apply stemming and drop stopwords\n",
    "    news = \" \".join(news)  # join the tokens into a string\n",
    "    news = lemm.lemmatize(news)  # apply lemmatization\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7h9LcSeOvqIf"
   },
   "outputs": [],
   "source": [
    "# encode categorical data to numerical\n",
    "df.loc[df['label']=='FAKE', 'label'] = 0    # FAKE = 0\n",
    "df.loc[df['label']=='REAL', 'label'] = 1    # REAL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ov32wf4addZQ"
   },
   "outputs": [],
   "source": [
    "# apply preprocessing to the news text\n",
    "df['processed_text'] = df['text'].apply(lambda news : pre_process_news(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "z1XaAs3Cde1m"
   },
   "outputs": [],
   "source": [
    "# seperate the processed data from labels\n",
    "data = df[\"processed_text\"]\n",
    "labels = df[\"label\"].to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MY8Kak6zdg2d"
   },
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts(texts = data)\n",
    "tokenized_data = tokenizer.texts_to_sequences(texts = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "P7sNlb3mdi9N"
   },
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "MAX_PAD_SEQUENCE_LENGTH = 3000\n",
    "data = pad_sequences(sequences= tokenized_data, maxlen = MAX_PAD_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7rJLrhGmYsW",
    "outputId": "1e6fe8ce-023d-4c04-abe5-50e3f2c364f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is (6335, 3000)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of the data is\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAdL8a0udkWl",
    "outputId": "28180f72-71a6-41c1-e336-44160675304f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train size: (4561, 3000)\n",
      "y_train size: (4561,)\n",
      "x_test size: (1267, 3000)\n",
      "y_test size: (1267,)\n",
      "x_valt size: (507, 3000)\n",
      "y_valt size: (507,)\n"
     ]
    }
   ],
   "source": [
    "# split dataset to training and testing with stratify\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
    "print(\"x_train size:\", x_train.shape)\n",
    "print(\"y_train size:\", y_train.shape)\n",
    "print(\"x_test size:\", x_test.shape)\n",
    "print(\"y_test size:\", y_test.shape)\n",
    "print(\"x_valt size:\", x_val.shape)\n",
    "print(\"y_valt size:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_06_J-zEdl6z"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe encoding\n",
    "file = open(embedding_path , encoding=\"utf8\")\n",
    "embedding_dict = {}\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    embedding_dict[values[0]] = values[1:]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-4ytOWwdrds",
    "outputId": "43bda56d-d96b-4ebb-9abb-682e16375b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of embedding matrix (45456, 50)\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix\n",
    "embedding_m = np.zeros((len(tokenizer.word_index) + 1, EMBEDDING_DIM)) # since we chose the embedding with 100 dimentions\n",
    "for word, indx in tokenizer.word_index.items():\n",
    "    vector = embedding_dict.get(word)\n",
    "    if vector:    # if there is no embedding for a word then it is left as 0 vector\n",
    "        embedding_m[indx] = vector\n",
    "print(\"The shape of embedding matrix\", embedding_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xQjsviYZvqIi"
   },
   "outputs": [],
   "source": [
    "# Convert the data to correct format and datatype\n",
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "x_val = np.asarray(x_val).astype(np.float32)\n",
    "y_val = np.asarray(y_val).astype(np.float32)\n",
    "x_test = np.asarray(x_test).astype(np.float32)\n",
    "y_test = np.asarray(y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCV7cdpt_MMJ"
   },
   "source": [
    "## Design\n",
    "\n",
    "For this project we utilize Convolutional Neural Network, Deep Neural Network, and Long Short-Term Memroy (Recurrent Neural Network). Below we use different neural network architectures to learn human writting patterns to classify news. The models use pre-trained GloVe encoding layers where each entry is encoded into a vector of 50 dimentions. Further explanation of the the neural network architecture model can be found below.\n",
    "\n",
    "**About Data**\n",
    "\n",
    "The training data was preprocessed and tokenized above. The tokenized dictionary contains key pairs of a unique number to a unique word.Each row of the dataset is a list of numbers, each containing the index value of the corresponding words in the tokenized dictionary. The pre-trained GloVe encoding is loaded and converted to a matrix. If the first index value in tokenized dictionary is \"men\" then the first row in the embedding matrix contains the embedding for the word \"men\". This allows us to use the index mapping from the tokenized dictionary for embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlyNy3d4vqIi"
   },
   "source": [
    "### CNN Model\n",
    "\n",
    "A convolutional neural network, or CNN, is a deep learninng network designed for processing structured arrays of data such as images. CNN are widely used for computer vision and natural language processing for text classification. CNN is a feed-forward neural network and its power comes from the convolutional layer. THe usage of the convolutional layers in a convolutional neural network mirros the structure of the human visual cortex where layers process an incoming input and identify complex features.\n",
    "\n",
    "The model below takes the data and embeds it on the GloVe embedding, followed by convolutional filter, maxpooling, flatening, and dense layers.\n",
    "\n",
    "* **Embedding Layer**: Turns positive integer into a dense vector of size 100. As shows below each work has a embedding. In the below picture word groot has embedding of 2 elements. In our case we have an embedding of 50 elements for each word.\n",
    "\n",
    "<img src=\"images/embedding.PNG\" alt=\"Embedding Visual\" style=\"width: 200px;\"/>\n",
    "\n",
    "* **Convolution Layer**: Creates a convolution kernel that is convolved with the layer input over a single spatial dimension to produce a tensor of outputs. Visual example below.\n",
    "\n",
    "<img src=\"images/Convolution.PNG\" alt=\"Convolution Visual\" style=\"width: 600px;\"/>\n",
    "\n",
    "* **Maxpooling Layer**: Downsamples the input representation by taking the maximum value over a spatial window of given size. Visual example below.\n",
    "\n",
    "<img src=\"images/maxpooling.png\" alt=\"Maxpooling Visual\" style=\"width: 600px;\"/>\n",
    "\n",
    "* **Flatten Layer**: Flattens the input meaning it converts a nxm matrix to a list of values. Visual example below.\n",
    "\n",
    "<img src=\"images/flattening.png\" alt=\"Flattening Visual\" style=\"width: 500px;\"/>\n",
    "\n",
    "* **Dense Layer**: It helps change the dimentionality of the output from preceding layer through the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kFrDq20Y41Oo"
   },
   "outputs": [],
   "source": [
    "def pred_acc(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    The function tests the trained model using test data and prints performance metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        model: the trained model to use for prediction making\n",
    "        x_test: test data\n",
    "        y_test: test data labels\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting to test the model...\")\n",
    "    y_preds = model.predict(x_test)\n",
    "    y_pred = np.round(y_preds).ravel()\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    c_pred = cm[0,0] + cm[1,1]\n",
    "    acc = accuracy_score(y_pred, y_test)\n",
    "    print(\"\\n# Correct predictions:\", c_pred)\n",
    "    print(\"\\n# Total tests attempted:\", len(y_test))\n",
    "    print(\"\\nAccuracy of the model: \", acc) \n",
    "    print('\\nConfusion Matrix:\\n', cm)\n",
    "    \n",
    "    print(\"classification Report\")\n",
    "    print(classification_report(y_test, y_preds, target_names = ['Fake','Real']))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    plt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "CGQwMGXnvqIj"
   },
   "outputs": [],
   "source": [
    "def grid_search(model_func, params, x_train, y_train):\n",
    "    \"\"\"\n",
    "      The applices using params on model_func, and tests it using x_tran and y_train.\n",
    "\n",
    "      Parameters:\n",
    "        model_func: model we want to apply grid search on\n",
    "        x_train: training data\n",
    "        y_train: training data labels\n",
    "        params: parameters we want to test\n",
    "\n",
    "      Returns:\n",
    "        returns gridsearch model which contains metrics from the GridSearch operation\n",
    "    \"\"\"\n",
    "    model = KerasClassifier(build_fn=model_func)\n",
    "    gs = GridSearchCV(model, params, cv=2, scoring = 'f1')\n",
    "    gs.fit(x_train, y_train)\n",
    "    \n",
    "    print(\"\\n\\n\\nThe best parameters\", gs.best_params_)\n",
    "    print(\"The best score for best parameters\", gs.best_score_)\n",
    "    \n",
    "    print(\"Grid Search Results\")\n",
    "    display(pd.DataFrame(gs.cv_results_))\n",
    "    \n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GANsj6i-cM93"
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "hPk1Bhnk-AtG"
   },
   "outputs": [],
   "source": [
    "def CNN_model():\n",
    "    \"\"\"\n",
    "    The function sets up the layers of the model.\n",
    "    \n",
    "    Parameters: \n",
    "        None\n",
    "    Returns:\n",
    "        built convolutional neural network model\n",
    "    \"\"\"\n",
    "    \n",
    "    cnn = Sequential(\n",
    "        [\n",
    "            Embedding(len(tokenizer.word_index) + 1, EMBEDDING_DIM, weights=[embedding_m], input_length=x_train.shape[1]),\n",
    "            Conv1D(32, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(16, 5, activation='relu'),\n",
    "            Flatten(),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    cnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    cnn.summary()\n",
    "\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkBOilSNvqIj"
   },
   "source": [
    "### CNN - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1I-BTMOjdQ7U",
    "outputId": "b538de22-33c2-4c3a-beb2-b23ec24c5227",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 599, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9520)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/7\n",
      "143/143 [==============================] - 13s 8ms/step - loss: 0.5840 - accuracy: 0.6820\n",
      "Epoch 2/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.3776 - accuracy: 0.8338\n",
      "Epoch 3/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.2049 - accuracy: 0.9285\n",
      "Epoch 4/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0940 - accuracy: 0.9675\n",
      "Epoch 5/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0376 - accuracy: 0.9868\n",
      "Epoch 6/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0341 - accuracy: 0.9904\n",
      "Epoch 7/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 599, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/7\n",
      "143/143 [==============================] - 2s 7ms/step - loss: 0.5653 - accuracy: 0.7050\n",
      "Epoch 2/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.3323 - accuracy: 0.8702\n",
      "Epoch 3/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.1645 - accuracy: 0.9404\n",
      "Epoch 4/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0783 - accuracy: 0.9768\n",
      "Epoch 5/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0226 - accuracy: 0.9943\n",
      "Epoch 6/7\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0289 - accuracy: 0.9943\n",
      "Epoch 7/7\n",
      "143/143 [==============================] - 1s 8ms/step - loss: 0.0096 - accuracy: 0.9987\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 599, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 2s 8ms/step - loss: 0.5803 - accuracy: 0.6987\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.3415 - accuracy: 0.8601\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.1654 - accuracy: 0.9404\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0870 - accuracy: 0.9711\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0232 - accuracy: 0.9952\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0165 - accuracy: 0.9952\n",
      "Epoch 7/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 9.8278e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 6.9936e-04 - accuracy: 1.0000\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 599, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 2s 8ms/step - loss: 0.5700 - accuracy: 0.7102\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.3369 - accuracy: 0.8562\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.1611 - accuracy: 0.9408\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0686 - accuracy: 0.9785\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0239 - accuracy: 0.9943\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0101 - accuracy: 0.9991\n",
      "Epoch 7/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9996\n",
      "Epoch 8/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 599, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/15\n",
      "143/143 [==============================] - 2s 7ms/step - loss: 0.5392 - accuracy: 0.7202\n",
      "Epoch 2/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.3203 - accuracy: 0.8702\n",
      "Epoch 3/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.1610 - accuracy: 0.9373\n",
      "Epoch 4/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0682 - accuracy: 0.9816\n",
      "Epoch 5/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0219 - accuracy: 0.9969\n",
      "Epoch 6/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0092 - accuracy: 0.9996\n",
      "Epoch 7/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 7.6975e-04 - accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 5.4379e-04 - accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 4.1264e-04 - accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 3.0706e-04 - accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 2.4210e-04 - accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.9522e-04 - accuracy: 1.0000\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 599, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/15\n",
      "143/143 [==============================] - 2s 10ms/step - loss: 0.5866 - accuracy: 0.6786\n",
      "Epoch 2/15\n",
      "143/143 [==============================] - 2s 12ms/step - loss: 0.3319 - accuracy: 0.8632\n",
      "Epoch 3/15\n",
      "143/143 [==============================] - 2s 12ms/step - loss: 0.1804 - accuracy: 0.9338\n",
      "Epoch 4/15\n",
      "143/143 [==============================] - 2s 12ms/step - loss: 0.0719 - accuracy: 0.9790\n",
      "Epoch 5/15\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 0.0265 - accuracy: 0.9921\n",
      "Epoch 6/15\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 0.0349 - accuracy: 0.9904\n",
      "Epoch 7/15\n",
      "143/143 [==============================] - 2s 12ms/step - loss: 0.0111 - accuracy: 0.9987\n",
      "Epoch 8/15\n",
      "143/143 [==============================] - 2s 11ms/step - loss: 0.0047 - accuracy: 0.9996\n",
      "Epoch 9/15\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 10/15\n",
      "143/143 [==============================] - 1s 9ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "143/143 [==============================] - 1s 9ms/step - loss: 9.5218e-04 - accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "143/143 [==============================] - 2s 11ms/step - loss: 6.6214e-04 - accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "143/143 [==============================] - 2s 12ms/step - loss: 5.3378e-04 - accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "143/143 [==============================] - 2s 11ms/step - loss: 3.9793e-04 - accuracy: 1.0000\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 599, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.6260 - accuracy: 0.6360\n",
      "Epoch 2/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4419 - accuracy: 0.8004\n",
      "Epoch 3/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2973 - accuracy: 0.8754\n",
      "Epoch 4/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1473 - accuracy: 0.9474\n",
      "Epoch 5/7\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0635 - accuracy: 0.9825\n",
      "Epoch 6/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0378 - accuracy: 0.9895\n",
      "Epoch 7/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0217 - accuracy: 0.9943\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 599, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_15 (Conv1D)          (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.6037 - accuracy: 0.6681\n",
      "Epoch 2/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3884 - accuracy: 0.8334\n",
      "Epoch 3/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2466 - accuracy: 0.9093\n",
      "Epoch 4/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1473 - accuracy: 0.9456\n",
      "Epoch 5/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0625 - accuracy: 0.9816\n",
      "Epoch 6/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0282 - accuracy: 0.9934\n",
      "Epoch 7/7\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0148 - accuracy: 0.9978\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPooling  (None, 599, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.6134 - accuracy: 0.6658\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4264 - accuracy: 0.8140\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2621 - accuracy: 0.9022\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1373 - accuracy: 0.9526\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0636 - accuracy: 0.9825\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0320 - accuracy: 0.9930\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0125 - accuracy: 0.9987\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0068 - accuracy: 0.9987\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0037 - accuracy: 0.9996\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPooling  (None, 599, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.5935 - accuracy: 0.6817\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3783 - accuracy: 0.8312\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2043 - accuracy: 0.9206\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0896 - accuracy: 0.9733\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0320 - accuracy: 0.9939\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0141 - accuracy: 0.9969\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0079 - accuracy: 0.9991\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0223 - accuracy: 0.9934\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0401 - accuracy: 0.9873\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0137 - accuracy: 0.9982\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_20 (Conv1D)          (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPoolin  (None, 599, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_21 (Conv1D)          (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/15\n",
      "72/72 [==============================] - 2s 12ms/step - loss: 0.5944 - accuracy: 0.6671\n",
      "Epoch 2/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3754 - accuracy: 0.8382\n",
      "Epoch 3/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2215 - accuracy: 0.9149\n",
      "Epoch 4/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1193 - accuracy: 0.9596\n",
      "Epoch 5/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0547 - accuracy: 0.9833\n",
      "Epoch 6/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0263 - accuracy: 0.9917\n",
      "Epoch 7/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0155 - accuracy: 0.9947\n",
      "Epoch 8/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0096 - accuracy: 0.9996\n",
      "Epoch 9/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 8.1121e-04 - accuracy: 1.0000\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_22 (Conv1D)          (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPoolin  (None, 599, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_23 (Conv1D)          (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/15\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.5862 - accuracy: 0.6900\n",
      "Epoch 2/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3730 - accuracy: 0.8360\n",
      "Epoch 3/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2250 - accuracy: 0.9141\n",
      "Epoch 4/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0995 - accuracy: 0.9676\n",
      "Epoch 5/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0428 - accuracy: 0.9917\n",
      "Epoch 6/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0200 - accuracy: 0.9961\n",
      "Epoch 7/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0110 - accuracy: 0.9982\n",
      "Epoch 8/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0068 - accuracy: 0.9996\n",
      "Epoch 9/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 9.6209e-04 - accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 7.4248e-04 - accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 5.9224e-04 - accuracy: 1.0000\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_24 (Conv1D)          (None, 2996, 32)          8032      \n",
      "                                                                 \n",
      " max_pooling1d_12 (MaxPoolin  (None, 599, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_25 (Conv1D)          (None, 595, 16)           2576      \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 9520)              0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 16)                152336    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435,761\n",
      "Trainable params: 2,435,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/10\n",
      "286/286 [==============================] - 4s 9ms/step - loss: 0.6368 - accuracy: 0.6562\n",
      "Epoch 2/10\n",
      "286/286 [==============================] - 3s 9ms/step - loss: 0.4731 - accuracy: 0.8492\n",
      "Epoch 3/10\n",
      "286/286 [==============================] - 2s 8ms/step - loss: 0.3276 - accuracy: 0.9259\n",
      "Epoch 4/10\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 0.1312 - accuracy: 0.9577\n",
      "Epoch 5/10\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 0.0531 - accuracy: 0.9860\n",
      "Epoch 6/10\n",
      "286/286 [==============================] - 2s 8ms/step - loss: 0.0258 - accuracy: 0.9934\n",
      "Epoch 7/10\n",
      "286/286 [==============================] - 2s 8ms/step - loss: 0.0154 - accuracy: 0.9980\n",
      "Epoch 8/10\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 0.0074 - accuracy: 0.9989\n",
      "Epoch 9/10\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 0.0043 - accuracy: 0.9993\n",
      "Epoch 10/10\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 0.0050 - accuracy: 0.9982\n",
      "The best parameters {'batch_size': 16, 'epochs': 10}\n",
      "The best score for best parameters 0.8861259257863294\n",
      "Grid Search Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-257723e6-2298-41b9-abea-b8f70256b893\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.538411</td>\n",
       "      <td>6.574603</td>\n",
       "      <td>0.426556</td>\n",
       "      <td>0.006765</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>{'batch_size': 16, 'epochs': 7}</td>\n",
       "      <td>0.880824</td>\n",
       "      <td>0.888698</td>\n",
       "      <td>0.884761</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.170536</td>\n",
       "      <td>5.068193</td>\n",
       "      <td>0.419487</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>{'batch_size': 16, 'epochs': 10}</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.899911</td>\n",
       "      <td>0.886126</td>\n",
       "      <td>0.013786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.433560</td>\n",
       "      <td>12.950714</td>\n",
       "      <td>0.423046</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>{'batch_size': 16, 'epochs': 15}</td>\n",
       "      <td>0.880847</td>\n",
       "      <td>0.890543</td>\n",
       "      <td>0.885695</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.778815</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>0.425003</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>{'batch_size': 32, 'epochs': 7}</td>\n",
       "      <td>0.856900</td>\n",
       "      <td>0.895575</td>\n",
       "      <td>0.876238</td>\n",
       "      <td>0.019338</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.452773</td>\n",
       "      <td>1.541527</td>\n",
       "      <td>0.535834</td>\n",
       "      <td>0.220447</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>{'batch_size': 32, 'epochs': 10}</td>\n",
       "      <td>0.869872</td>\n",
       "      <td>0.875906</td>\n",
       "      <td>0.872889</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.288697</td>\n",
       "      <td>4.931318</td>\n",
       "      <td>0.360769</td>\n",
       "      <td>0.062208</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>{'batch_size': 32, 'epochs': 15}</td>\n",
       "      <td>0.883925</td>\n",
       "      <td>0.880889</td>\n",
       "      <td>0.882407</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-257723e6-2298-41b9-abea-b8f70256b893')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-257723e6-2298-41b9-abea-b8f70256b893 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-257723e6-2298-41b9-abea-b8f70256b893');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      17.538411      6.574603         0.426556        0.006765   \n",
       "1      16.170536      5.068193         0.419487        0.003051   \n",
       "2      29.433560     12.950714         0.423046        0.004342   \n",
       "3       5.778815      0.009105         0.425003        0.000119   \n",
       "4       9.452773      1.541527         0.535834        0.220447   \n",
       "5      17.288697      4.931318         0.360769        0.062208   \n",
       "\n",
       "  param_batch_size param_epochs                            params  \\\n",
       "0               16            7   {'batch_size': 16, 'epochs': 7}   \n",
       "1               16           10  {'batch_size': 16, 'epochs': 10}   \n",
       "2               16           15  {'batch_size': 16, 'epochs': 15}   \n",
       "3               32            7   {'batch_size': 32, 'epochs': 7}   \n",
       "4               32           10  {'batch_size': 32, 'epochs': 10}   \n",
       "5               32           15  {'batch_size': 32, 'epochs': 15}   \n",
       "\n",
       "   split0_test_score  split1_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.880824           0.888698         0.884761        0.003937   \n",
       "1           0.872340           0.899911         0.886126        0.013786   \n",
       "2           0.880847           0.890543         0.885695        0.004848   \n",
       "3           0.856900           0.895575         0.876238        0.019338   \n",
       "4           0.869872           0.875906         0.872889        0.003017   \n",
       "5           0.883925           0.880889         0.882407        0.001518   \n",
       "\n",
       "   rank_test_score  \n",
       "0                3  \n",
       "1                1  \n",
       "2                2  \n",
       "3                5  \n",
       "4                6  \n",
       "5                4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#gridsearch on CNN\n",
    "params = {'epochs': [7, 10, 15],\n",
    "          'batch_size': [16, 32]}\n",
    "gs_cnn = grid_search(CNN_model, params, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19LjYoedcF2z"
   },
   "source": [
    "### DNN\n",
    "\n",
    "Deep neural network is also known as fast forward neural network as the data will only flow in forward direction. DNN structure contains an input layer, a output layer and multiple hidden layers. DNN is a somple neural network that processsed data in a complex ways by employing sophisticated math modeling. \n",
    "\n",
    "Deep neural network below utalizes embedding, densem and convolution layers as described above. Additionally it uses GlobalMaxPooling, which allows the AI to use the golbal maximum value/features during the computations.\n",
    "\n",
    "\n",
    "![DNN visual](images/DNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "w5a1iJYVSk4v"
   },
   "outputs": [],
   "source": [
    "def DNN_model():\n",
    "    \"\"\"\n",
    "    The function sets up the layers of the model.\n",
    "    \n",
    "    Parameters: \n",
    "        None\n",
    "    Returns:\n",
    "        built deep neural network model\n",
    "    \"\"\"\n",
    "    dnn = Sequential(\n",
    "      [\n",
    "          Embedding(len(tokenizer.word_index) + 1, EMBEDDING_DIM, weights=[embedding_m], input_length=x_train.shape[1]),\n",
    "          Conv1D(128, 3, activation='relu'),\n",
    "          GlobalMaxPooling1D(),\n",
    "          Dense(64, activation='relu'),\n",
    "          Dense(1, activation='sigmoid')\n",
    "      ])\n",
    "    dnn.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "    dnn.summary()\n",
    "    return dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ned0pZ7dvqIl"
   },
   "source": [
    "### DNN - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TqGAdjQNvqIl",
    "outputId": "89fb1f33-c972-4f71-91b0-4a675955b88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_26 (Conv1D)          (None, 2998, 128)         19328     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,300,449\n",
      "Trainable params: 2,300,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 2s 29ms/step - loss: 0.6150 - accuracy: 0.6759\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.4258 - accuracy: 0.8070\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.3131 - accuracy: 0.8825\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.2325 - accuracy: 0.9193\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1851 - accuracy: 0.9461\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1474 - accuracy: 0.9548\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1030 - accuracy: 0.9746\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0742 - accuracy: 0.9833\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0517 - accuracy: 0.9930\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0380 - accuracy: 0.9947\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_14 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_27 (Conv1D)          (None, 2998, 128)         19328     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,300,449\n",
      "Trainable params: 2,300,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 2s 29ms/step - loss: 0.5997 - accuracy: 0.6879\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.4159 - accuracy: 0.8220\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.2979 - accuracy: 0.8948\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.2256 - accuracy: 0.9272\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.1815 - accuracy: 0.9391\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.1279 - accuracy: 0.9702\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0912 - accuracy: 0.9790\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0629 - accuracy: 0.9899\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0411 - accuracy: 0.9939\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0301 - accuracy: 0.9956\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_15 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_28 (Conv1D)          (None, 2998, 128)         19328     \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,300,449\n",
      "Trainable params: 2,300,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/15\n",
      "36/36 [==============================] - 2s 27ms/step - loss: 0.6400 - accuracy: 0.6500\n",
      "Epoch 2/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.4367 - accuracy: 0.8039\n",
      "Epoch 3/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.3217 - accuracy: 0.8754\n",
      "Epoch 4/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.2399 - accuracy: 0.9224\n",
      "Epoch 5/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.2035 - accuracy: 0.9254\n",
      "Epoch 6/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.1345 - accuracy: 0.9662\n",
      "Epoch 7/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.1075 - accuracy: 0.9711\n",
      "Epoch 8/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0778 - accuracy: 0.9816\n",
      "Epoch 9/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0540 - accuracy: 0.9895\n",
      "Epoch 10/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0361 - accuracy: 0.9908\n",
      "Epoch 11/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0230 - accuracy: 0.9965\n",
      "Epoch 12/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0169 - accuracy: 0.9969\n",
      "Epoch 13/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0115 - accuracy: 0.9974\n",
      "Epoch 14/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0051 - accuracy: 0.9991\n",
      "Epoch 15/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0093 - accuracy: 0.9965\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_16 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_29 (Conv1D)          (None, 2998, 128)         19328     \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,300,449\n",
      "Trainable params: 2,300,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/15\n",
      "36/36 [==============================] - 2s 27ms/step - loss: 0.6146 - accuracy: 0.6541\n",
      "Epoch 2/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.4099 - accuracy: 0.8176\n",
      "Epoch 3/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.3144 - accuracy: 0.8733\n",
      "Epoch 4/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.2367 - accuracy: 0.9220\n",
      "Epoch 5/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.1739 - accuracy: 0.9483\n",
      "Epoch 6/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.1383 - accuracy: 0.9588\n",
      "Epoch 7/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.1041 - accuracy: 0.9719\n",
      "Epoch 8/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0722 - accuracy: 0.9838\n",
      "Epoch 9/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0485 - accuracy: 0.9917\n",
      "Epoch 10/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0347 - accuracy: 0.9912\n",
      "Epoch 11/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0222 - accuracy: 0.9965\n",
      "Epoch 12/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0164 - accuracy: 0.9961\n",
      "Epoch 13/15\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0069 - accuracy: 0.9996\n",
      "Epoch 15/15\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0112 - accuracy: 0.9974\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_17 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_30 (Conv1D)          (None, 2998, 128)         19328     \n",
      "                                                                 \n",
      " global_max_pooling1d_4 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,300,449\n",
      "Trainable params: 2,300,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 2s 26ms/step - loss: 0.6374 - accuracy: 0.6522\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.4430 - accuracy: 0.8158\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.3375 - accuracy: 0.8684\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.2577 - accuracy: 0.9035\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.2020 - accuracy: 0.9342\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1569 - accuracy: 0.9531\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1153 - accuracy: 0.9732\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0857 - accuracy: 0.9825\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0580 - accuracy: 0.9895\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0370 - accuracy: 0.9956\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0268 - accuracy: 0.9952\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0187 - accuracy: 0.9961\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0086 - accuracy: 0.9991\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0077 - accuracy: 0.9987\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0111 - accuracy: 0.9969\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0057 - accuracy: 0.9991\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 6.4034e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 7.4123e-04 - accuracy: 1.0000\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_18 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_31 (Conv1D)          (None, 2998, 128)         19328     \n",
      "                                                                 \n",
      " global_max_pooling1d_5 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,300,449\n",
      "Trainable params: 2,300,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 2s 27ms/step - loss: 0.6188 - accuracy: 0.6629\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.4029 - accuracy: 0.8334\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.3163 - accuracy: 0.8654\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.2420 - accuracy: 0.9097\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.1957 - accuracy: 0.9329\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 0.1312 - accuracy: 0.9654\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.1045 - accuracy: 0.9715\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.0673 - accuracy: 0.9877\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 0.0541 - accuracy: 0.9851\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.0297 - accuracy: 0.9943\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.0224 - accuracy: 0.9961\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0243 - accuracy: 0.9912\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0119 - accuracy: 0.9969\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0070 - accuracy: 0.9978\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0094 - accuracy: 0.9965\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 5.9417e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 3.9193e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0062 - accuracy: 0.9978\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_19 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_32 (Conv1D)          (None, 2998, 128)         19328     \n",
      "                                                                 \n",
      " global_max_pooling1d_6 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,300,449\n",
      "Trainable params: 2,300,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/15\n",
      "72/72 [==============================] - 3s 27ms/step - loss: 0.5573 - accuracy: 0.7088\n",
      "Epoch 2/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.3427 - accuracy: 0.8546\n",
      "Epoch 3/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.2384 - accuracy: 0.9094\n",
      "Epoch 4/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.1811 - accuracy: 0.9347\n",
      "Epoch 5/15\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.1297 - accuracy: 0.9579\n",
      "Epoch 6/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.0940 - accuracy: 0.9717\n",
      "Epoch 7/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.0679 - accuracy: 0.9792\n",
      "Epoch 8/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.0372 - accuracy: 0.9917\n",
      "Epoch 9/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.0294 - accuracy: 0.9912\n",
      "Epoch 10/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.0164 - accuracy: 0.9956\n",
      "Epoch 11/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.0111 - accuracy: 0.9967\n",
      "Epoch 12/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.0064 - accuracy: 0.9980\n",
      "Epoch 13/15\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.0053 - accuracy: 0.9974\n",
      "Epoch 14/15\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 15/15\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.0021 - accuracy: 0.9996\n",
      "The best parameters {'batch_size': 64, 'epochs': 15}\n",
      "The best score for best parameters 0.9106092151029948\n",
      "Grid Search Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9014a721-0ed2-4322-b7c5-608300339a20\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.030955</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.715661</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>{'batch_size': 64, 'epochs': 10}</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.904660</td>\n",
       "      <td>0.896775</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.268493</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>0.633594</td>\n",
       "      <td>0.083005</td>\n",
       "      <td>64</td>\n",
       "      <td>15</td>\n",
       "      <td>{'batch_size': 64, 'epochs': 15}</td>\n",
       "      <td>0.912049</td>\n",
       "      <td>0.909169</td>\n",
       "      <td>0.910609</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.628574</td>\n",
       "      <td>0.696088</td>\n",
       "      <td>0.718364</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>{'batch_size': 64, 'epochs': 20}</td>\n",
       "      <td>0.919842</td>\n",
       "      <td>0.894279</td>\n",
       "      <td>0.907061</td>\n",
       "      <td>0.012782</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9014a721-0ed2-4322-b7c5-608300339a20')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9014a721-0ed2-4322-b7c5-608300339a20 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9014a721-0ed2-4322-b7c5-608300339a20');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      11.030955      0.006216         0.715661        0.000770   \n",
       "1      21.268493      0.009506         0.633594        0.083005   \n",
       "2      20.628574      0.696088         0.718364        0.000343   \n",
       "\n",
       "  param_batch_size param_epochs                            params  \\\n",
       "0               64           10  {'batch_size': 64, 'epochs': 10}   \n",
       "1               64           15  {'batch_size': 64, 'epochs': 15}   \n",
       "2               64           20  {'batch_size': 64, 'epochs': 20}   \n",
       "\n",
       "   split0_test_score  split1_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.888889           0.904660         0.896775        0.007886   \n",
       "1           0.912049           0.909169         0.910609        0.001440   \n",
       "2           0.919842           0.894279         0.907061        0.012782   \n",
       "\n",
       "   rank_test_score  \n",
       "0                3  \n",
       "1                1  \n",
       "2                2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {'epochs': [10, 15, 20],\n",
    "          'batch_size': [64]}\n",
    "\n",
    "gs_dnn = grid_search(DNN_model, params, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp3rhle4cXY4"
   },
   "source": [
    "### LSTM\n",
    "\n",
    "Long-short term memory(LSTM) is an advanced architecture of recurrent neural network(RNN) which handles situations where RNN fails. LSTM are developed to deal with the vanishing gradient problem that can be encountered while training tradition RNNs. It is a special network that is capable of learning long term dependencies in data. An LSM helps the information to flow through the units without being altered by allowing only a few linear interactions.\n",
    "\n",
    "<img src=\"images/LSTM.PNG\" alt=\"LSTM Visual\" style=\"width: 500px;\"/>\n",
    "\n",
    "The below LSTM model utalizes Embedding, LSTM and Dense layers only as it requires a lot of processing power. Similary to above models we compile the model using binary_crossentropy loss and adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "in-fZyXMcY3r"
   },
   "outputs": [],
   "source": [
    "def LSTM_model():\n",
    "    \"\"\"\n",
    "    The function sets up the layers of the model.\n",
    "    \n",
    "    Parameters: \n",
    "        None\n",
    "    Returns:\n",
    "        built bidirectional long short-term memory model\n",
    "    \"\"\"\n",
    "    lstm = Sequential(\n",
    "         [\n",
    "              Embedding(len(tokenizer.word_index) + 1, EMBEDDING_DIM, weights=[embedding_m], input_length=x_train.shape[1]),\n",
    "              LSTM(units=128),\n",
    "              Dense(64, activation='relu'),\n",
    "              Dense(1, activation='sigmoid')\n",
    "         ])\n",
    "    lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    lstm.summary()\n",
    "\n",
    "    return  lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7d1Ah8fvqIl"
   },
   "source": [
    "### LSTM - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kSUeKLrtvqIm",
    "outputId": "ddbd40c5-2b2f-42f3-f81f-a2b793113b4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_20 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               91648     \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,372,769\n",
      "Trainable params: 2,372,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "36/36 [==============================] - 7s 134ms/step - loss: 0.6183 - accuracy: 0.6592\n",
      "Epoch 2/6\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 0.5008 - accuracy: 0.7662\n",
      "Epoch 3/6\n",
      "36/36 [==============================] - 7s 195ms/step - loss: 0.4138 - accuracy: 0.8232\n",
      "Epoch 4/6\n",
      "36/36 [==============================] - 5s 133ms/step - loss: 0.3374 - accuracy: 0.8632\n",
      "Epoch 5/6\n",
      "36/36 [==============================] - 6s 174ms/step - loss: 0.2798 - accuracy: 0.8961\n",
      "Epoch 6/6\n",
      "36/36 [==============================] - 5s 134ms/step - loss: 0.2156 - accuracy: 0.9197\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_21 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               91648     \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,372,769\n",
      "Trainable params: 2,372,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "36/36 [==============================] - 6s 133ms/step - loss: 0.6133 - accuracy: 0.6528\n",
      "Epoch 2/6\n",
      "36/36 [==============================] - 5s 133ms/step - loss: 0.4993 - accuracy: 0.7628\n",
      "Epoch 3/6\n",
      "36/36 [==============================] - 8s 217ms/step - loss: 0.4166 - accuracy: 0.8211\n",
      "Epoch 4/6\n",
      "36/36 [==============================] - 6s 167ms/step - loss: 0.3351 - accuracy: 0.8685\n",
      "Epoch 5/6\n",
      "36/36 [==============================] - 6s 179ms/step - loss: 0.2791 - accuracy: 0.8935\n",
      "Epoch 6/6\n",
      "36/36 [==============================] - 6s 166ms/step - loss: 0.1924 - accuracy: 0.9329\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_22 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               91648     \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,372,769\n",
      "Trainable params: 2,372,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "36/36 [==============================] - 7s 132ms/step - loss: 0.5958 - accuracy: 0.6912\n",
      "Epoch 2/12\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.4841 - accuracy: 0.7737\n",
      "Epoch 3/12\n",
      "36/36 [==============================] - 5s 129ms/step - loss: 0.3912 - accuracy: 0.8329\n",
      "Epoch 4/12\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 0.3927 - accuracy: 0.8434\n",
      "Epoch 5/12\n",
      "36/36 [==============================] - 5s 131ms/step - loss: 0.3269 - accuracy: 0.8702\n",
      "Epoch 6/12\n",
      "36/36 [==============================] - 5s 131ms/step - loss: 0.2279 - accuracy: 0.9154\n",
      "Epoch 7/12\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.1680 - accuracy: 0.9368\n",
      "Epoch 8/12\n",
      "36/36 [==============================] - 5s 133ms/step - loss: 0.1222 - accuracy: 0.9601\n",
      "Epoch 9/12\n",
      "36/36 [==============================] - 5s 136ms/step - loss: 0.0977 - accuracy: 0.9675\n",
      "Epoch 10/12\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.0664 - accuracy: 0.9807\n",
      "Epoch 11/12\n",
      "36/36 [==============================] - 5s 131ms/step - loss: 0.0373 - accuracy: 0.9912\n",
      "Epoch 12/12\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.0204 - accuracy: 0.9952\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_23 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               91648     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,372,769\n",
      "Trainable params: 2,372,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "36/36 [==============================] - 6s 133ms/step - loss: 0.6010 - accuracy: 0.6677\n",
      "Epoch 2/12\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.4741 - accuracy: 0.7751\n",
      "Epoch 3/12\n",
      "36/36 [==============================] - 5s 131ms/step - loss: 0.3881 - accuracy: 0.8369\n",
      "Epoch 4/12\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.3229 - accuracy: 0.8742\n",
      "Epoch 5/12\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 0.2671 - accuracy: 0.9005\n",
      "Epoch 6/12\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.2135 - accuracy: 0.9237\n",
      "Epoch 7/12\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 0.1397 - accuracy: 0.9522\n",
      "Epoch 8/12\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.1002 - accuracy: 0.9623\n",
      "Epoch 9/12\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 0.0717 - accuracy: 0.9768\n",
      "Epoch 10/12\n",
      "36/36 [==============================] - 5s 131ms/step - loss: 0.0365 - accuracy: 0.9895\n",
      "Epoch 11/12\n",
      "36/36 [==============================] - 5s 131ms/step - loss: 0.0216 - accuracy: 0.9943\n",
      "Epoch 12/12\n",
      "36/36 [==============================] - 5s 136ms/step - loss: 0.0245 - accuracy: 0.9921\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_24 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 128)               91648     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,372,769\n",
      "Trainable params: 2,372,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 6s 133ms/step - loss: 0.6008 - accuracy: 0.6759\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.4784 - accuracy: 0.7671\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 5s 134ms/step - loss: 0.4814 - accuracy: 0.7860\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 5s 133ms/step - loss: 0.3897 - accuracy: 0.8355\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 0.2872 - accuracy: 0.8895\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 5s 134ms/step - loss: 0.2186 - accuracy: 0.9175\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 5s 134ms/step - loss: 0.1557 - accuracy: 0.9474\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.1063 - accuracy: 0.9671\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 5s 134ms/step - loss: 0.0625 - accuracy: 0.9807\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 5s 137ms/step - loss: 0.0476 - accuracy: 0.9860\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.0255 - accuracy: 0.9939\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 5s 134ms/step - loss: 0.0329 - accuracy: 0.9899\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 5s 139ms/step - loss: 0.0137 - accuracy: 0.9978\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.0077 - accuracy: 0.9987\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.0053 - accuracy: 0.9991\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 0.0034 - accuracy: 0.9991\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 4.8097e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 3.4559e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 5s 131ms/step - loss: 2.5670e-04 - accuracy: 1.0000\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_25 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 128)               91648     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,372,769\n",
      "Trainable params: 2,372,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 8s 133ms/step - loss: 0.6106 - accuracy: 0.6580\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 6s 160ms/step - loss: 0.5012 - accuracy: 0.7650\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 9s 235ms/step - loss: 0.3938 - accuracy: 0.8312\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 5s 131ms/step - loss: 0.3315 - accuracy: 0.8680\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.2847 - accuracy: 0.8965\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 5s 133ms/step - loss: 0.2411 - accuracy: 0.9154\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 5s 142ms/step - loss: 0.1449 - accuracy: 0.9505\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 5s 136ms/step - loss: 0.0996 - accuracy: 0.9662\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 5s 134ms/step - loss: 0.0618 - accuracy: 0.9798\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.0323 - accuracy: 0.9934\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.0599 - accuracy: 0.9820\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 5s 136ms/step - loss: 0.0282 - accuracy: 0.9934\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 0.0133 - accuracy: 0.9978\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 5s 136ms/step - loss: 0.0072 - accuracy: 0.9991\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 5s 133ms/step - loss: 0.0057 - accuracy: 0.9991\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 5s 134ms/step - loss: 0.0044 - accuracy: 0.9991\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 5s 138ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 7.0793e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 5s 133ms/step - loss: 4.2346e-04 - accuracy: 1.0000\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_26 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 128)               91648     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,372,769\n",
      "Trainable params: 2,372,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "72/72 [==============================] - 13s 160ms/step - loss: 0.5525 - accuracy: 0.7211\n",
      "Epoch 2/12\n",
      "72/72 [==============================] - 10s 133ms/step - loss: 0.4419 - accuracy: 0.8150\n",
      "Epoch 3/12\n",
      "72/72 [==============================] - 9s 131ms/step - loss: 0.3702 - accuracy: 0.8461\n",
      "Epoch 4/12\n",
      "72/72 [==============================] - 9s 132ms/step - loss: 0.3184 - accuracy: 0.8724\n",
      "Epoch 5/12\n",
      "72/72 [==============================] - 9s 132ms/step - loss: 0.2585 - accuracy: 0.9057\n",
      "Epoch 6/12\n",
      "72/72 [==============================] - 9s 131ms/step - loss: 0.1866 - accuracy: 0.9347\n",
      "Epoch 7/12\n",
      "72/72 [==============================] - 9s 131ms/step - loss: 0.1652 - accuracy: 0.9423\n",
      "Epoch 8/12\n",
      "72/72 [==============================] - 9s 132ms/step - loss: 0.0929 - accuracy: 0.9695\n",
      "Epoch 9/12\n",
      "72/72 [==============================] - 10s 141ms/step - loss: 0.0561 - accuracy: 0.9807\n",
      "Epoch 10/12\n",
      "72/72 [==============================] - 10s 132ms/step - loss: 0.0330 - accuracy: 0.9910\n",
      "Epoch 11/12\n",
      "72/72 [==============================] - 9s 131ms/step - loss: 0.0235 - accuracy: 0.9939\n",
      "Epoch 12/12\n",
      "72/72 [==============================] - 9s 132ms/step - loss: 0.0144 - accuracy: 0.9967\n",
      "The best parameters {'batch_size': 64, 'epochs': 12}\n",
      "The best score for best parameters 0.8544401196331828\n",
      "Grid Search Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1098a417-a041-48d7-813f-5d3001c6a535\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.119032</td>\n",
       "      <td>3.758735</td>\n",
       "      <td>3.962258</td>\n",
       "      <td>0.508204</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>{'batch_size': 64, 'epochs': 6}</td>\n",
       "      <td>0.816568</td>\n",
       "      <td>0.830068</td>\n",
       "      <td>0.823318</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.042257</td>\n",
       "      <td>11.799048</td>\n",
       "      <td>4.486746</td>\n",
       "      <td>0.991414</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>{'batch_size': 64, 'epochs': 12}</td>\n",
       "      <td>0.862450</td>\n",
       "      <td>0.846430</td>\n",
       "      <td>0.854440</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121.566849</td>\n",
       "      <td>23.376482</td>\n",
       "      <td>4.499442</td>\n",
       "      <td>0.981037</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>{'batch_size': 64, 'epochs': 20}</td>\n",
       "      <td>0.841242</td>\n",
       "      <td>0.853893</td>\n",
       "      <td>0.847568</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1098a417-a041-48d7-813f-5d3001c6a535')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1098a417-a041-48d7-813f-5d3001c6a535 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1098a417-a041-48d7-813f-5d3001c6a535');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      39.119032      3.758735         3.962258        0.508204   \n",
       "1      72.042257     11.799048         4.486746        0.991414   \n",
       "2     121.566849     23.376482         4.499442        0.981037   \n",
       "\n",
       "  param_batch_size param_epochs                            params  \\\n",
       "0               64            6   {'batch_size': 64, 'epochs': 6}   \n",
       "1               64           12  {'batch_size': 64, 'epochs': 12}   \n",
       "2               64           20  {'batch_size': 64, 'epochs': 20}   \n",
       "\n",
       "   split0_test_score  split1_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.816568           0.830068         0.823318        0.006750   \n",
       "1           0.862450           0.846430         0.854440        0.008010   \n",
       "2           0.841242           0.853893         0.847568        0.006325   \n",
       "\n",
       "   rank_test_score  \n",
       "0                3  \n",
       "1                1  \n",
       "2                2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {'epochs': [6, 12, 20],\n",
    "          'batch_size': [64]}\n",
    "gs_lstm = grid_search(LSTM_model, params, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tests were ran multiple times on free version of Google Colab GPU as I did not have access to Nvidia GPU. The above tests were done multiple times to determine the the best combination of layers that give the best performance. The above output is the best combinaation of layers, which was found after some testing. What can be learned from the above data is that feed forward networks like DNN and CNN require a large quantity of labeled data to truly shine. This also requires much higher computation power as well. The runtime and the performace of the models can be observed above in the gridsearch process at well as teh table displayed at the end of the gridsearch process.\n",
    "\n",
    "The google colab uses Python 3 Google Compute Engine backend(GPU) with limited RAM and GPU access.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGczc21pz8la",
    "outputId": "56ae9262-1b9f-4574-a9fa-b2cce56b6dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_29 (Embedding)    (None, 3000, 50)          2272800   \n",
      "                                                                 \n",
      " conv1d_35 (Conv1D)          (None, 2998, 128)         19328     \n",
      "                                                                 \n",
      " global_max_pooling1d_9 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,300,449\n",
      "Trainable params: 2,300,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " \n",
      "Epoch 1/15\n",
      "45/45 [==============================] - 4s 61ms/step - loss: 0.5849 - accuracy: 0.6995 - val_loss: 0.4513 - val_accuracy: 0.8170\n",
      "Epoch 2/15\n",
      "45/45 [==============================] - 2s 50ms/step - loss: 0.3856 - accuracy: 0.8404 - val_loss: 0.3556 - val_accuracy: 0.8502\n",
      "Epoch 3/15\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.2869 - accuracy: 0.8884 - val_loss: 0.2952 - val_accuracy: 0.8754\n",
      "Epoch 4/15\n",
      "45/45 [==============================] - 2s 52ms/step - loss: 0.2238 - accuracy: 0.9155 - val_loss: 0.3071 - val_accuracy: 0.8738\n",
      "Epoch 5/15\n",
      "45/45 [==============================] - 2s 50ms/step - loss: 0.1810 - accuracy: 0.9358 - val_loss: 0.2429 - val_accuracy: 0.8975\n",
      "Epoch 6/15\n",
      "45/45 [==============================] - 2s 52ms/step - loss: 0.1356 - accuracy: 0.9581 - val_loss: 0.3198 - val_accuracy: 0.8659\n",
      "Epoch 7/15\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.1127 - accuracy: 0.9653 - val_loss: 0.2450 - val_accuracy: 0.9054\n",
      "Epoch 8/15\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.0818 - accuracy: 0.9765 - val_loss: 0.2135 - val_accuracy: 0.9180\n",
      "Epoch 9/15\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.0643 - accuracy: 0.9835 - val_loss: 0.2052 - val_accuracy: 0.9227\n",
      "Epoch 10/15\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.0446 - accuracy: 0.9891 - val_loss: 0.2627 - val_accuracy: 0.9038\n",
      "Epoch 11/15\n",
      "45/45 [==============================] - 2s 52ms/step - loss: 0.0384 - accuracy: 0.9893 - val_loss: 0.2125 - val_accuracy: 0.9259\n",
      "Epoch 12/15\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.0213 - accuracy: 0.9960 - val_loss: 0.2085 - val_accuracy: 0.9259\n",
      "Epoch 13/15\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0165 - accuracy: 0.9961 - val_loss: 0.4659 - val_accuracy: 0.8423\n",
      "Epoch 14/15\n",
      "45/45 [==============================] - 2s 52ms/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 0.2235 - val_accuracy: 0.9322\n",
      "Epoch 15/15\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2193 - val_accuracy: 0.9259\n"
     ]
    }
   ],
   "source": [
    "# Train the best model using the best parameters on all 90% data and 10% validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.1, random_state=42, stratify=labels)\n",
    "\n",
    "# Convert the data to correct format and datatype\n",
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "x_val = np.asarray(x_val).astype(np.float32)\n",
    "y_val = np.asarray(y_val).astype(np.float32)\n",
    "\n",
    "# re-train the model\n",
    "dnn = DNN_model()\n",
    "history_dnn = dnn.fit(x_train, y_train, validation_data = (x_val, y_val), epochs=15, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEQYVXqndmTJ"
   },
   "source": [
    "## Diagnosis\n",
    "\n",
    "The models learn about the human writting patterns to determine is a news is real or false as such it would fail the classification if the fake news was also generated by an AI. The model would also fail to correctly classify if the user attemps to classify other types of news such as sports. Each of the models above utilizes pre-trained GloVe encoding which might also contain bias. \n",
    "\n",
    "**Scoping**- The simple principle behind this project is to decrease the effects the fake news has in the works and allow the users to gain confidence in the information they consume. An area of source harm would be the bias found in labeled news dataset or the bias from GloVe embedding.\n",
    " \n",
    "**Mapping**- The false positive classification could lead to the change in the environment around user such the early retirement of a political leader or sudden change in the stock prices which leads to product prices going up. False negative would lead to similar effects like voting for the wrong official.\n",
    " \n",
    "**Artifact Collection**- The assumption made for the dataset is that it does not contain bias. The requires labeled data is availabe virtually at large quantity.\n",
    "\n",
    "**Testing**- The model would be built after extensive testing using neural networks such as CNN, DNN, LSTM. We also test the models with diverse news data. \n",
    "\n",
    "To futher improve the model we could train a different types of news like politics, sports, etc.  We could also have multiple experts in a field label the dataset so as to avoid bias. Another option is to get more data as some neural network require large quantity of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ya0DDCViuMq"
   },
   "source": [
    "## Deployment\n",
    "A simple strategy  to deploy the model is to create an API that could be accesed by anyone easily. We could consider using serverless AWS server to create and API and make predictions using API. Another option is to create an restful API using flask and and make API request for a prediction. For the deployment of this project A flask AI was created which can be found in the FlaskServer, server.py file. A simple GET call to confirm the connection to the server, as the POST call used used to take the input from the news input from the user to make a prediction on. The below picture depicts the deployment cycle. The model can be updated and deployed to GitHub for version control as well as being published to DockerHub to make it avaibale to the user. This process can be improved using my creating an API using cloud services uploading the saved model on cloud as discussed earlier.\n",
    "\n",
    "<img src=\"images/deployment.JPG\" alt=\"model deployment\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80zeRd10vqIm"
   },
   "source": [
    "### Save info - for predition steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6qmIUBZivjy",
    "outputId": "7233ca03-6691-4892-c8c5-a497aaa21d56"
   },
   "outputs": [],
   "source": [
    "# Save keras model\n",
    "dnn.save(filepath = save_keras_model_path)\n",
    "\n",
    "#Save embedding dictionary\n",
    "with open(save_tokenizer_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXG6gJkFvqIm"
   },
   "source": [
    "### Test Flask API\n",
    "\n",
    "Read the readme.md for instructions on how to run the flask server\n",
    "\n",
    "**_Note_** : This notebook should be running inside the container for a connection to flask server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iL6AXzXWvqIn"
   },
   "outputs": [],
   "source": [
    "# Copy oase the news article in the following format  text = \"\"\" news \"\"\". As show below.\n",
    "text = \"\"\"\n",
    "\ttext\n",
    "9\tHillary Clinton and Donald Trump made some inaccurate claims during an NBC “commander-in-chief” forum on military and veterans issues:\n",
    "\n",
    "• Clinton wrongly claimed Trump supported the war in Iraq after it started, while Trump was wrong, once again, in saying he was against the war before it started.\n",
    "\n",
    "• Trump said that President Obama set a “certain date” for withdrawing troops from Iraq, when that date was set before Obama was sworn in.\n",
    "\n",
    "• Trump said that Obama’s visits to China, Saudi Arabia and Cuba were “the first time in the history, the storied history of Air Force One” when “high officials” of a host country did not appear to greet the president. Not true.\n",
    "\n",
    "• Clinton said that Trump supports privatizing the Veterans Health Administration. That’s false. Trump said he supports allowing veterans to seek care at either public or private hospitals.\n",
    "\n",
    "• Trump said Clinton made “a terrible mistake on Libya” when she was secretary of State. But, at the time, Trump also supported U.S. action that led to the removal of Moammar Gadhafi from power.\n",
    "\n",
    "• Trump cherry-picked Clinton’s words when he claimed Clinton said “vets are being treated, essentially, just fine.” Clinton had said the problems in the Department of Veterans Affairs were not as “widespread” as some Republicans claimed, but she went on to acknowledge problems, including the issue of wait times for doctors.\n",
    "\n",
    "The forum, sponsored by NBC News and the Iraq and Afghanistan Veterans of America, was held Sept. 7 at the Intrepid Sea, Air & Space Museum in New York City. Today show host Matt Lauer, and members of the military and veterans in the audience, questioned the candidates separately.\n",
    "\n",
    "Trump said he “was totally against the war in Iraq,” while Clinton claimed that he supported the Iraq War before and after it started. The facts don’t support either candidate’s strong assertions.\n",
    "\n",
    "Our review of Trump’s statements before and after the Iraq War started found no evidence that Trump opposed the war before it started. In fact, he expressed mild support for invading Iraq when asked about it on the Howard Stern radio show on Sept. 11, 2002 — about six months before the war started.\n",
    "\n",
    "Stern asked Trump if he supported a war with Iraq, and Trump responded, “Yeah, I guess so.”\n",
    "\n",
    "In the NBC commander in chief forum, Trump cited an Esquire article that appeared in August 2004 to show his opposition to the war. But that article appeared 17 months after the war started.\n",
    "\n",
    "As for Clinton, who as a senator voted in October 2002 to authorize the war in Iraq, the Democratic nominee claimed that Trump “supported it before it happened, he supported it as it was happening and he is on record as supporting it after it happened.”\n",
    "\n",
    "But just as there is no evidence that Trump opposed the Iraq War before it started, the Clinton campaign offered no evidence that Trump supported the war “after it happened.”\n",
    "\n",
    "The Clinton campaign cited Trump’s interview on March 21, 2003, with Neil Cavuto of Fox Business just two days after the war started.\n",
    "\n",
    "Cavuto asked Trump about the impact of the war on the stock market. Trump said the war “looks like a tremendous success from a military standpoint,” and he predicted the market will “go up like a rocket” after the war. But Cavuto does not ask Trump whether the U.S. should have gone to war with Iraq or whether he supports the war, and Trump doesn’t offer an opinion.\n",
    "\n",
    "As early as July 2003, Trump expressed concern on Hardball with Chris Matthews about money being spent in Iraq rather than in the U.S. Two months later, Trump told MSNBC’s Joe Scarborough, “I guess maybe if I had to do it, I would have fought terrorism but not necessarily Iraq.”\n",
    "\n",
    "Clinton invited her audience to read Trump’s comments on the Iraq War. They can read our timeline, “Donald Trump and the Iraq War.”\n",
    "\n",
    "Trump said President Obama set a “certain date” for withdrawing troops from Iraq, but that date was actually set by President George W. Bush.\n",
    "\n",
    "NBC’s Matt Lauer asked Trump about his tendency to respond, when pushed for details on his military proposals, that he’s not going to give details because he wants to be “unpredictable.” Trump responded, “Absolutely,” and went on to criticize Obama for revealing the withdrawal date.\n",
    "\n",
    "As we said then, Republicans and Democrats disagree on whether Obama or Bush is to blame for withdrawing all combat troops from Iraq at the end of 2011. But that date was set when Bush signed the Status of Forces Agreement on Dec. 14, 2008. It said: “All the United States Forces shall withdraw from all Iraqi territory no later than December 31, 2011.”\n",
    "\n",
    "In the NBC forum, Trump also called the withdrawal of troops “a terrible decision.” As we’ve explained before, Condoleezza Rice, Bush’s secretary of State, later wrote that Bush wanted an agreement for a residual force to remain, but Iraqi Prime Minister Nouri al-Maliki objected.\n",
    "\n",
    "Once Obama took office in January 2009, he had three years to renegotiate the deal, which his administration tried to do, to leave a residual American troop force. But Maliki still didn’t agree. Negotiations broke down in October 2011 over the issue of whether U.S. troops would be shielded from criminal prosecution by Iraqi authorities. Whether Obama did enough is a matter of opinion: His then defense secretary, Leon Panetta, later wrote that the president didn’t press hard enough for a deal. But some experts say Iraq was more closely aligned at the time with Iran and there wasn’t a deal to be made with Maliki.\n",
    "\n",
    "So, both presidents had a role in the withdrawal of troops. But Trump wrongly said that Obama was the one who set a “certain date” for withdrawal and let U.S. enemies know about it, when that date was set before Obama was sworn in.\n",
    "\n",
    "It’s worth noting that Trump said in a March 16, 2007, interview on CNN that the troops should be withdrawn quickly from Iraq.\n",
    "\n",
    "Trump said that Obama’s visits to China, Saudi Arabia and Cuba were “the first time in the history, the storied history of Air Force One” when “high officials” of a host country did not appear to greet the president.\n",
    "\n",
    "That’s not true. Other presidents have encountered similar low-key greetings on foreign trips aboard the presidential aircraft.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example client below for Flask GET and POST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bj57DTinvqIn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response {'label': 'Successful connection to server.'}\n"
     ]
    }
   ],
   "source": [
    "# Test the connection with the flask server\n",
    "response = requests.get(\"http://127.0.0.1:5001/\")\n",
    "print('response', response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c-_7WZ-PvqIo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response {'prediction': 'Real'}\n"
     ]
    }
   ],
   "source": [
    "# Make the POST request to predict the validity of the news.\n",
    "data = {'news': text}\n",
    "response = requests.post(url=\"http://127.0.0.1:5001/\", data = data)\n",
    "print('response', response.json() if str(response) != '<Response [500]>' else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K22YRl1vqIo"
   },
   "source": [
    "### References\n",
    "[1] C. K. Hiramath and G. C. Deshpande, \"Fake News Detection Using Deep Learning Techniques,\" 2019 1st International Conference on Advances in Information Technology (ICAIT), 2019, pp. 411-415, doi: 10.1109/ICAIT47043.2019.8987258.\n",
    "\n",
    "[2] A. J. Keya, S. Afridi, A. S. Maria, S. S. Pinki, J. Ghosh and M. F. Mridha, \"Fake News Detection Based on Deep Learning,\" 2021 International Conference on Science & Contemporary Technologies (ICSCT), 2021, pp. 1-6, doi: 10.1109/ICSCT53883.2021.9642565.\n",
    "\n",
    "[3] M. Umer, Z. Imtiaz, S. Ullah, A. Mehmood, G. S. Choi and B. -W. On, \"Fake News Stance Detection Using Deep Learning Architecture (CNN-LSTM),\" in IEEE Access, vol. 8, pp. 156695-156706, 2020, doi: 10.1109/ACCESS.2020.3019735.\n",
    "\n",
    "[4] T. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, and A. Kalai, “Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings,” arXiv:1607.06520 [cs, stat], Jul. 2016, Accessed: May 08, 2022. [Online]. Available: https://arxiv.org/abs/1607.06520?msclkid=14e6b1decea211eca7c294a30bddb6e3\n",
    "\n",
    "[5] K. Kim and C. Jeong, \"Fake News Detection System using Article Abstraction,\" 2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE), 2019, pp. 209-212, doi: 10.1109/JCSSE.2019.8864154.\n",
    "\n",
    "[6] A. Khalil, M. Jarrah, M. Aldwairi and Y. Jararweh, \"Detecting Arabic Fake News Using Machine Learning,\" 2021 Second International Conference on Intelligent Data Science Technologies and Applications (IDSTA), 2021, pp. 171-177, doi: 10.1109/IDSTA53674.2021.9660811.\n",
    "\n",
    "[7] C. K. Hiramath and G. C. Deshpande, \"Fake News Detection Using Deep Learning Techniques,\" 2019 1st International Conference on Advances in Information Technology (ICAIT), 2019, pp. 411-415, doi: 10.1109/ICAIT47043.2019.8987258.\n",
    "\n",
    "[8] T. Jiang, J. P. Li, A. U. Haq and A. Saboor, \"Fake News Detection using Deep Recurrent Neural Networks,\" 2020 17th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP), 2020, pp. 205-208, doi: 10.1109/ICCWAMTIP51612.2020.9317325.\n",
    "\n",
    "[9] N. Smitha and R. Bharath, \"Performance Comparison of Machine Learning Classifiers for Fake News Detection,\" 2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA), 2020, pp. 696-700, doi: 10.1109/ICIRCA48905.2020.9183072.\n",
    "\n",
    "[10] B. Majumdar, M. RafiuzzamanBhuiyan, M. A. Hasan, M. S. Islam and S. R. H. Noori, \"Multi Class Fake News Detection using LSTM Approach,\" 2021 10th International Conference on System Modeling & Advancement in Research Trends (SMART), 2021, pp. 75-79, doi: 10.1109/SMART52563.2021.9676333.\n",
    "\n",
    "[11] P. Shrivastava and D. K. Sharma, \"Fake Content Identification Using Pre-Trained Glove-Embedding,\" 2021 5th International Conference on Information Systems and Computer Networks (ISCON), 2021, pp. 1-6, doi: 10.1109/ISCON52037.2021.9702379.\n",
    "\n",
    "[12] A. Kishwar and A. Zafar, \"Predicting Fake News using GloVe and BERT Embeddings,\" 2021 6th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM), 2021, pp. 1-6, doi: 10.1109/SEEDA-CECNSM53056.2021.9566243."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FakeNewsDetection (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
